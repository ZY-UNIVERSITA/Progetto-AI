## Panoramica della rete (v12)

| Voce | Valore |
|------|--------|
| Architettura | Conv2d+BN+ReLU(**32-32**) → MP → Conv2d+BN+ReLU(**64-64**) → MP → Conv2d+BN+ReLU(**128-128**) → GAP 1x1 → FC 128 → FC(out) |
| Parametri | **307k** |
| Optimizer | Adam (lr 1 e-5 → warm-up a 5 e-4) |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 ||
| Epoch eseguiti | 18 (early-stop) |

---

### Andamento addestramento

| Ep | LR | Acc. train | Acc. val | Loss train | Loss val |
|----|----|-----------|---------|------------|----------|
| 1  | 1e-5   | 4.5% | 5.9%  | 3.38 | 3.34 |
| 5  | 2.3e-4 | 68.3% | 93.3% | 1.31 | 0.64 |
| 7  | 5e-4   | 91.3% | 96.65%| 0.31 | 0.114 |
| 11 | 5e-4   | 97.5% | 99.12%| 0.097| 0.038 |
| 13 | 5e-4   | 98.4% | 99.56%| 0.068| 0.015 |
| 16 | 5e-4   | 98.2% | 99.38%| 0.065| 0.026 |
| 17 | 5e-5   | 99.28%| **99.78%**| 0.032| **0.009** |
| 18 | 5e-5   | 99.59%| 99.65%| 0.024| 0.010 |

---

### Osservazioni chiave

1. **Doppie conv per blocco**: feature estratte meglio senza gonfiare i parametri tramite doppio kernel che permette di catturare più dettagli in ogni blocco prima di passare a catturare le relazioni a più alto livello.  

2. **Regolarizzazione**: BN in ogni conv + Dropout 0.3 -> overfitting quasi nullo con una differenza di 0.5 punti tra val e train.  

3. **Scheduler**: StepLR riduce nella parte finali il LR per raffinare i pesi.

---

### Confronto tra le versioni migliori

|  | v8 | v11 | **v12** |
|---|---|---|---|
| Parametri | 552 k | 113 k | **308 k** |
| Val-Acc | 99.43% | 97.18% | **99.78%** |
| Val-Loss | 0.023 | 0.112 | **0.009** |
| Gap T-V | 0.31% | 5.2% | **0.5%** |

v12 si presenta come il migliore modello
