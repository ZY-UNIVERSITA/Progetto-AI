## Panoramica della rete (v7)

| Chiave | Valore |
|--------|--------|
| Architettura | **2x** Conv-BN-ReLU-Pool **(16 → 32)** → Flatten → Dropout 0.3 → FC 128 → Dropout 0.3 → FC(out) |
| Parametri totali | **1M** (–75% rispetto a v6 (4M), con aumento di parametri in conv) |
| Optimizer | Adam - lr (1e-5 → 5e-4) |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento addestramento

```text
        Acc. train  Acc. val    Loss train      Loss val  
E1      15.6%       44.3%       3.14            2.75 
E5      87.9%       92.6%       0.43            0.27 
E10     97.9%       97.4%       0.07            0.09 
E16     98.5%       98.1%       0.05            0.076 
E20     99.6%       98.68%      0.013           0.059 
E24     99.7%       98.90%      0.011           0.053
```

---

### Osservazioni chiave

1. **Profondità convoluzionale aumentato**  
   • 2 blocchi conv+bn+max_pool applicano 2 layer di kernel da 16 a 32 canali in successione.  
   • Dopo 2 MaxPool, feature-map di dimensioni 16x16: la rete vede contesti sia dettagli fini sia le relazioni tra di loro.

2. **Flatten molto più piccolo**  
   • Questo grazie al max pooling che riduce le dimensionalità.  
   • Il numero di parametri nel FC layer scendono del 75% da 4M a 1M.  

3. **Generalizzazione**  
   • Val-loss crolla a 0.05 vs 0.18 di v6.  
   • Gap train-val < 1% → overfitting quasi sparito.

4. **Efficienza**  
   • Parametri ridotti di 75% rispetto a v6, tempo/epoch ≈ 28–35 s (più veloce di v6 e simile a v4-v5).

---

### 4. Confronto storico

|  | v3 | v5 | v6 | **v7** |
|---|---|---|---|---|
| Conv stack | 1 | 1 | 1 (32ch) | 2 (16-32) |
| Parametri | 2.1 M | 2.1 M | 4.2 M | **1.06 M** |
| Val-acc | 96.52% | 96.39% | 96.08% | **98.90%** |
| Val-loss | 0.153 | 0.161 | 0.180 | **0.053** |
| Train loss | 0.03  | 0.018 | 0.016 | **0.011** |
| Train acc | 99.17% | 99.50% | 99.54% | **99.7%**  |
| Gap T-V | 3.3% | 3.1% | 3.4% | **0.8%** |

Questo ha permessi di avere più di 2.8 punti percentuale di accuracy in val rispetto alla versione precedente e un miglioramento del 65% di loss sul validation rispetto alla migliore loss del modello precedente.
