## Panoramica della rete (v0)

| Chiave | Valore |
|--------|--------|
| Architettura | Conv-BN-ReLU-Pool (16→32→64) → **AdaptiveAvgPool 1x1** → Flatten → Dropout 0.3 → FC 128 → Dropout 0.3 → FC(out) |
| Parametri totali | **35 k** (v8 = 552k, 93% in meno di parametri) |
| Optimizer | Adam (lr 1 e-5 → warm-up a 5 e-4) |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento addestramento

| Ep | LR | Acc-train | Acc-val | Loss-train | Loss-val |
|----|----|-----------|---------|------------|----------|
| 1 | 1 e-5 | 3.1% | 3.1% | 3.44 | 3.41 |
| 5 | 2.3 e-4 | 17.6% | 43.6% | 2.77 | 2.31 |
| 10| 5 e-4  | 64.8% | 84.5% | 0.99 | 0.61 |
| 16| 5 e-4  | 74.9% | 89.2% | 0.69 | 0.38 |
| 20| 5 e-5  | 78.2% | 90.6% | 0.60 | 0.34 |
| 21| 5 e-5  | 79.7% | 91.45% | 0.585 | 0.326 |
| 24| 5 e-5  | 78.9% | 91.1% | 0.585 | 0.331 |

---

## Osservazioni chiave

1. **Riduzione estrema della capacità**  
    - AdaptiveAvgPool produce un vettore di **64 feature**; FC 128 aggiunge appena 8k pesi → il modello ha il 93% di parametri in meno di v8.  
    - Train-acc rimane < 80% con peggioramento senza segni di miglioramento verso le ultime epoche.
    - Il numero basso di parametri può spiegare dei risultati di training base ed è causato dal fatto che il modello non abbia abbastanza capacità espressiva per catturare le relazioni all'interno dell'immagine.
