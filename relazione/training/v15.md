## Panoramica della rete (v15 – 44 classi)

| Chiave | Valore |
|--------|--------|
| Architettura | 2×Conv32 → MP → 2×Conv64 → MP → 2×Conv128 → GAP 1×1 → FC 128 → Dropout 0.3 → FC 44 |
| Parametri totali | 309k |
| Pesi di partenza | checkpoint v14 (30 classi) – ultimo layer ri-inizializzato a 44 neuroni |
| Dataset train | • 30 classi originali + AUG <br>• 14 classi nuove + AUG |
| Optimizer | Adam – lr iniz. 1 e-6 |
| Warm-up | 5 epoch → lr 1 e-5 |
| Scheduler | StepLR(step 10, γ 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento addestramento

| Ep | LR | Acc-train | Acc-val | Loss-train | Loss-val |
|----|----|-----------|---------|------------|----------|
| 1  | 1e-6   |  4.1% | 12.8% | 3.96 | 3.38 |
| 5  | 6.3e-6 | 48.3% | 92.4% | 2.03 | 1.04 |
| 10 | 1e-5   | 91.1% | 99.60% | 0.39 | 0.027 |
| 14 | 1e-5   | 95.2% | 99.70% | 0.20 | 0.016 |
| 17 | 1e-6   | 96.6% | 99.65% | 0.14 | 0.015 |
| 22 | 1e-6   | 96.73%| **99.76%** | 0.135| **0.013** |
| 24 | 1e-6   | 96.84%| 99.70% | 0.131| 0.013 |

---

## Osservazioni chiave

1. **Transfer learning efficace**  
    - Nonostante 14 classi nuove, la rete raggiunge subito > 90% di acc-val (ep 5) grazie alle feature già apprese.  

2. **Warm-up e lr bassi**  
    - LR massimo solo 1e-5: nessuna oscillazione; miglioramento graduale fino al plateau (ep 14-24).
    - Rischio minimizzato di modificare troppo i pesi e perdere le conoscenze pregresse infatti l'obiettivo è aggiornare la parte convoluzionale sulle nuove classi mantenendo quello imparato fino ad ora.
 
3. **Dataset misto**
    - Il dataset comprende sia dati pre-esistenti sia dati nuovi, questo evita il catastrophic forgetting. 

---

## Confronto tra versioni

|  | v13 | v14 | **v15 (44 cls + AUG)** |
|---|---|---|---|
| Classi | 30 | 30 | **44** |
| Val Accuracy | 99.78% | 99.78% | **99.76%** |
| Val Loss | 0.009 | 0.012 | **0.013** |
| Train Accuracy | 99.28% | 97.38% | **96.73%** |
| Gap T-V | 0.5% | 2.4% | **3.0%** |

**Interpretazione**  
- A parità di architettura, passare da 30→44 classi con data-augmentation e LR più basso ha mantenuto l’accuratezza alta che pero è più basso così come il loss rispetto alle versione precedenti, segno di minore confidenza sulle nuove categorie.  

