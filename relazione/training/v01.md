## Panoramica della rete (v1)

| Chiave | Valore |
|------|--------|
| Architettura | 1x Conv(16) → ReLU → MaxPool → Flatten → FC(128) → ReLU → FC(out) |
| Parametri totali | ≈ 2,1 M |
| Immagini | 64x64, 1 canale |
| Optimizer | Adam - lr iniziale di 1e-5 |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento dell'addestramento

```text
        Acc. train  Acc. val    Loss train  Loss val
E5      78.5%      84.3%      0.80        0.64
E10     99.2%      92.4%      0.03        0.30
E15     99.8%      93.8%      0.01        0.28
E20     100%       94.3%      0.004       0.27
E24     100%       94.6%      0.002       0.27
```

Osservazioni chiave  
1. **Fase di “warm-up”** (E1-E4): il lr cresce in maniera esponenziale e si passa da 15% a 70% di accuracy val in 4 epoch.  
2. **Fase di crescita stabile** (E4-E10): lr 0.001 → si arriva rapidamente oltre il 90% di acc. val.  
3. **Plateau / leggero overfitting** (E10-E24): la loss train crolla quasi a 0, la acc. val si assesta fra 93-94%. Gap training-validazione ≈ 5-6% con segno di overfitting iniziale.
4. **StepLR** a E10 riduce lr a 1e-4 per rifinire i pesi.  
5. **Early-stopping**: con un criterio di stop del 5% in 10 valutazioni, il training si interrompe a E24.
