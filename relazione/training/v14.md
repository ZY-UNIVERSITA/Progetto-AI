## Panoramica della rete (v14 – fine-tuning con data-augmentation)

Il modello di base pre-addesetrato usato è il modello v13.

| Chiave | Valore |
|--------|--------|
| Architettura | Conv(32,32) → MP → Conv(64,64) → MP → Conv(128,128) → GAP 1x1 → FC 128 → FC(out) |
| Parametri | 307k|
| Peso di partenza | checkpoint v13 |
| Dataset | data-augmentation (flip, rotate, ecc.) del train dataset originale |
| Optimizer | Adam (lr iniz. 1e-6) |
| Warm-up | 5 epoch 1e-6 → 1e-5 |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento dell'addestramento

| Ep | LR | Acc-train | Acc-val | Loss-train | Loss-val |
|----|----|-----------|---------|------------|----------|
| 1  | 1e-6  | 83.64% | 99.47% | 0.58 | 0.058 |
| 5  | 6.3e-6 | 91.94% | 99.47% | 0.31 | 0.047 |
| 10 | 1e-5  | 95.59% | 99.60% | 0.18 | 0.027 |
| 14 | 1e-5  | 96.74% | 99.65% | 0.13 | 0.016 |
| 17 | 1e-6  | 97.34% | 99.65% | 0.107| 0.014 |
| 23 | 1e-6  | 97.38% | **99.78%** | 0.103| **0.012** |
| 24 | 1e-6  | 97.45% | 99.74% | 0.101| 0.013 |

---

## Osservazioni chiave

1. **Fine-tuning** 
    - Già alla 1° epoch il modello mantiene alte perfomrance grazie ai pesi ereditati.  
    - Train accuracy parte bassa (83%) e cresce lentamente: l'augmentation rende i campioni più difficili con meno over-fitting e più generalizzazione mentre il val dataset vede campioni più semplici.

2. **Learning-rate molto ridotto (10x inferiore)**  
    - Viene usato per evitare cambiamenti dei pesi troppo aggressivi quando il modello è già stato addestrato precedentemente.

3. **Risultato finale**  
    - Val-acc 99.78% uguaglia il record di v13.  
    - Val-loss 0.012 rimane leggermente superiore a 0.009 di v13.
    - Train-acc 97.4% (< 99% di v13) indica buona regolarizzazione su un dataset augmentato con strutture mai viste prima.

---

## Confronto tra versioni

|  | v12 | v13 | **v14 (DA + LR 1e-5)** |
|---|---|---|---|
| Val accuracy | **99.78%** | 99.60% | **99.78%** |
| Val loss | **0.009** | 0.051 | 0.012 |
| Train accuracy | **99.28%** | 98.04% | 97.38% |
| Gap T-V | 0.5% | 1.56% | **2.40%** |
| Oscillazioni | moderate | minime | minime |

### Interpretazione  
Data-augmentation + LR ridotto preservano l'accuratezza top, riducono le oscillazioni e garantiscono un modello meno “confidente” (loss un po' più alta) ma potenzialmente più robusto su variazioni delle immagini. 

