## Panoramica della rete (v13)

| Chiave | Valore |
|------|--------|
| Architettura | Conv(32,32) → MP → Conv(64,64) → MP → Conv(128,128) → GAP 1x1 → FC 128 → FC(out) |
| Parametri totali | 307 710 |
| Optimizer | Adam (lr iniz. 1 e-5) |
| Warm-up | 5 epoch 1 e-5 → **1 e-4** |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 29 (early-stop) |

---

## Andamento dell’addestramento

| Ep | LR | Acc-train | Acc-val | Loss-train | Loss-val |
|----|----|-----------|---------|------------|----------|
| 5  | 6.3e-5 | 38.7% | 55.2% | 2.40 | 1.99 |
| 7  | 1e-4   | 72.0% | 90.4% | 1.24 | 0.86 |
| 12 | 1e-4   | 92.9% | 98.68%| 0.33 | 0.162 |
| 16 | 1e-4   | 96.4% | 98.19%| 0.18 | 0.095 |
| 17 | 1e-5   | 97.4% | 99.51%| 0.147| 0.055 |
| 20 | 1e-5   | 98.0% | **99.60%** | 0.131| **0.051** |
| 24 | 1e-5   | 98.1% | 99.56%| 0.119| 0.046 |
| 29 | 1e-6   | 98.3% | 99.60%| 0.112| 0.043 |

---

## Osservazioni chiave

1. Crescita più **graduale**: con LR-picco 1e-4 servono 12 epoch per superare il 98% di acc-val (v12 lo faceva in 7 epoche). 
 
2. Dopo lo step (ep 17) la curva si stabilizza senza oscillazioni, ma la **loss-val resta 5-6x più alta** (0.05 vs 0.009 ad E17).  

3. **Addestramento più stabile**: la LR più bassa stabilizza il training che cresce più dolcemente senza oscillazioni troppo evidenti. Ne risente però le metriche che potrebbero richiedere più epoche per convergere completamente a causa del LR più basso.