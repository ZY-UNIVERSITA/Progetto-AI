## Panoramica della rete (v8)

| Chiave | Valore |
|--------|--------|
| Architettura | Conv-BN-ReLU-Pool **(16 → 32 → 64)** → Flatten → Dropout 0.3 → FC 128 → ReLU → Dropout 0.3 → FC(out) |
| Parametri totali | **≈ 552 k** (-48% rispetto a v7) |
| Optimizer | Adam - lr (1e-5 → 5e-4) |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |
---

## Andamento dell'addestramento

| Ep | LR | Acc. train | Acc. val | Loss train | Loss val |
|----|----|-----------|---------|------------|----------|
| 1  | 1.0 e-5 | 10.8% | 31.7% | 3.26 | 2.99 |
| 5  | 2.3 e-4 | 88.7% | 94.6% | 0.40 | 0.219 |
| 10 | 5.0 e-4 | 98.35% | 98.77% | 0.058 | 0.041 |
| 15 | 5.0 e-4 | 98.56% | 98.90% | 0.040 | 0.040 |
| 20 | 5.0 e-5 | 99.69% | 99.38% | 0.011 | 0.024 |
| 24 | 5.0 e-5 | 99.80% | 99.21% | 0.0079 | 0.0238 |

---

## Osservazioni chiave

1. **Profondità convoluzionale**: (3 blocchi) + BN → feature più robuste con pochi parametri. La rete è in grado di vedere sempre più dettagli e come questi si relazionano tra di loro ad alto livello.

2. **Flatten ridotto**: 64 x 8 x 8 = 4096 neuroni all'uscita dal layer di convoluzione → meno peso sui fully-connected.  

3. **Overfitting**: Gap train-val < 0.4%: overfitting quasi annullato grazie a Dropout + BN e ai layer aggiuntivi.  

4. **LR**: il warmup graduale + LR di picco 5e-4 e successiva riduzione del LR ha evitato oscillazioni importanti durante l'aggiornamento dei pesi.

---

### Confronto tra v7 e v8

|  | v7 | **v8** |
|---|---|---|
| Parametri | 1.06 M | **0.55 M** |
| Val-acc | 98.90% | **99.43%** |
| Val-loss | 0.053 | **0.023** |
| Gap T-V | 0.81% | **0.31%** |

Riducendo ancora il numero di pesi e aumentando quelli sulla convoluzioni si ha un guadagno di **+0.53%** di accuracy e ridottoa la loss del 50%.
