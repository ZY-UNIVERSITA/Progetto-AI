## Panoramica della rete (v2)

| Chiave | Valore |
|------|--------|
| Architettura | 1 x Conv(16) → ReLU → MaxPool → Flatten → **Dropout 0.3** → FC(128) → ReLU → FC(out) |
| Parametri totali | ≈ 2,1 M |
| Immagini | 64 x 64 - 1 canale |
| Optimizer | Adam - lr iniz. 1e-5 con warmup fino 1e-3 |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 29 (early-stop) |

---

## Andamento dell'addestramento

```text
        Acc. train      Acc. val        Loss train      Loss val
E5      79.8%           84.8%           0.77            0.60
E10     98.2%           93.1%           0.06            0.27
E15     98.1%           93.3%           0.06            0.29
E20     99.9%           95.6%           0.005           0.21
E25     100%            95.7%           0.003           0.21
E29     100%            95.8%           0.002           0.21
```

### Osservazioni chiave  
1. **Fase di “warm-up”** (E1-E4) – il learning-rate cresce in maniera esponenziale: l'accuracy di validazione passa da 19% a 73%.  

2. **Crescita rapida** (E4-E10) – con lr = 0.001 si supera velocemente il 90% di acc. val.  

3. **Dropout efficace** – rispetto alla v1 il gap train-val scende ad una distanza di 3-4%; la loss di validazione cala del 22% (0.27 → 0.21).  

4. **StepLR** a fine E10 c'è una riduzione del lr piccoli miglioramenti di 2 punti percentuali fino a circa 95.8%.  

5. **Plateau** (E20-E29): val_accuracy oscilla intorno al 95%, loss quasi piatta a 0.21 per diverse epoche.  

6. **Early-stopping** – criterio di miglioramento ≥ 5% in 10 valutazioni: il training si arresta a E29 perché il miglioramento è insufficiente.