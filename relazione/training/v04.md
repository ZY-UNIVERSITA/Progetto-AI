## Panoramica della rete (v4)

| Chiave | Valore |
|------|--------|
| Architettura | Conv(16) → **BatchNorm** → ReLU → MaxPool → Flatten → Dropout 0.3 → FC(128) → ReLU → Dropout 0.3 → FC(out) |
| Parametri totali | ≈ 2 101 342 |
| Data  | 64x64 - 1 canale |
| Optimizer | Adam - lr iniz. 1e-5 |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento addestramento 

```text
        Acc. train      Acc. val        Loss train      Loss val
E1      20.5%           53.3%           2.96            2.41
E5      81.4%           88.3%           0.61            0.45
E10     96.0%           93.4%           0.11            0.25  
E16     97.7%           92.7%           0.07            0.29   
E20     99.4%           96.3%           0.02            0.16
E24     99.4%           96.34%          0.021           0.156 
```

### Osservazioni chiave
1. **BatchNorm accelera l'apprendimento**: già alla prima epoca la val-acc è 53% (v3 stava al 13%) e a E3 si è all'80%. L'addestramento ha visto un accelerata.  

2. **Overfitting in aumento**: gap train-val ≈ 3% (più basso di v1/v2, lievemente più alto di v3). La batch norm potrebbe aumentare la capacità del modello di imparare meglio dati dati senza però portare ad una maggiore generalizzazione.

3. **Val-loss stabile**: 0.21 → 0.15; la discesa è più lenta rispetto a v3, ma più regolare. Arriva agli stessi livelli di v3.

---

## 3. Confronto fra le 4 release

|  | v1 | v2 | v3 | **v4** |
|---|---|---|---|---|
| Extra layer | – | +Dropout | +2xDropout | +BatchNorm |
| Val Accuracy | 94.58% | 95.86% | **96.52%** | 96.34% |
| Val Loss | 0.269 | 0.210 | **0.153** | 0.156 |
| Train Accuracy | **100%** | 99.97% | 99.17% | 99.40% |
| Gap train-val | 5.4% | 4.1% | 3.35% | **3.06%**|
| Best epoch | 24 | 29 | 24 | 24 |
