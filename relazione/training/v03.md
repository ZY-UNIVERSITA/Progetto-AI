## Panoramica della rete (v3)

| Chiave | Valore |
|------|--------|
| Architettura | Conv(16) → ReLU → MaxPool → Flatten → Dropout 0.3 → FC(128) → ReLU → **Dropout 0.3** → FC(out) |
| Parametri totali | ≈ 2,1 M |
| Immagini | 64 x 64 - 1 canale |
| Optimizer | Adam - lr iniz. 1e-5 |
| Scheduler | StepLR(step = 10, γ = 0.1) |
| Batch | 32 |
| Epoch previsti | 50 |
| Epoch eseguiti | 24 (early-stop) |

---

## Andamento dell'addestramento

```text
      Acc. train   Acc. val    Loss train  Loss val
E5    72.0%       82.7%      1.02        0.68
E10   95.1%       94.6%      0.15        0.21
E15   96.9%       95.4%      0.09        0.18
E20   98.9%       96.2%      0.04        0.16
E24   99.2%       96.5%      0.03        0.15  
```

---

## Osservazioni chiave  
1. **Doppio Dropout**  
   - gap train-val ridotto (≈ 3.7% di differenza) dal 5% precedente. 
   - Le metriche di train e val iniziali sono peggiori della v2, causato dal doppio dropout che richiede alla rete uno sforzo ulteriore per generalizzare.

2. **Epoche successive** (E10-E24): la rete impara velocemente e dalla E10 in poi con la riduzione del lr, si affinano i pesi e permette di guadagnare all'incirca 2 punti % di acc_val e la loss_val di 0.05 nelle 10 epoche successive senza troppe oscillazioni.

3. **Early-stopping**: forse sono possibili ulteriori epoche di addestramento in quanto i valori potrebbero ancora migliorare e non è proprio in un punto di plateau.

---

## Confronto fra le tre versioni

|  | v1 | v2 | v3 |
|---|---|---|---|
| Dropout | ✗ | 1 x FC | 2 x FC |
| Val Accuracy | 94.58% | 95.86% | **96.52%** |
| Val Loss | 0.269 | 0.210 | **0.153** |
| Train Accuracy | 100% | 99.97% | **99.17%** |
| Gap train-val | ~5.4% | ~3.9% | **~2.6%** |
| Epoch finali | 24 | 29 | 24 |

In pratica, ogni incremento di regolarizzazione, tramite dropout, ha portato:  
   - riduzione progressiva dell'overfitting  
   - abbassamento della loss di validazione (-43% v1 → v2 e ‑27% v2 → v3)  
   - +1.94% di accuracy val complessiva rispetto alla baseline (v1).
